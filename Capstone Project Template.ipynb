{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project is to analyze US immigration and tourism data. To achieve this goal, the data is extracted from different datasets and transformed to work in a star schema. This schema is then ready to be used to observe trends and behaviors in the data over time.\n",
    "\n",
    "The project was developed according to the following steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#step1)\n",
    "* [Step 2: Explore and Assess the Data](#step2)\n",
    "* [Step 3: Define the Data Model](#step3)\n",
    "* [Step 4: Run ETL to Model the Data](#step4)\n",
    "* [Step 5: Complete Project Write Up](#step5)\n",
    "* [Step 6: Data Analysis Demo over the processed tables](#step6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.0') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.executor.memory', '12g') \\\n",
    "    .config('spark.driver.memory', '12g') \\\n",
    "    .config('spark.memory.offHeap.enabled', True) \\\n",
    "    .config('spark.memory.offHeap.size', '12g') \\\n",
    "    .config('spark.sql.shuffle.partitions', 64) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "- Explain what you plan to do in the project in more detail:\n",
    "    - Import data from US Immigration Data, Airlines and Global Temperature datasets\n",
    "    - Load the data into pySpark dataframes to transform the data accordingly\n",
    "    - Create a star schema and populate the fact and dimensions with data from the datasets\n",
    "    - Store the data in parquet files that can be used in a Data Warehouse for data analysis purposes\n",
    "- What data do you use?\n",
    "    - I94 US Immigration Data - 2016\n",
    "    - World Airports Data\n",
    "    - World Tempearture Data\n",
    "- What is your end solution look like?\n",
    "    - Tables stored in a star schema built to perform data analysis on immigration data in the US\n",
    "- What tools did you use?\n",
    "    - Python 3.8 (dependencies listed in requirements.txt)\n",
    "    - Pyspark 3.3.0 (Scala version 2.12.15)\n",
    "    - OpenJDK 64-Bit Server VM (build 11.0.15+10-Ubuntu-0ubuntu0.20.04.1, mixed mode, sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I94 Immigration Data\n",
    "\n",
    "This data was provided by the US National Tourism and Trade Office and contains statistics of visitor arrival during 2016.\n",
    "\n",
    "The original dataset is in SAS7BDAT format and can be found [here](https://www.trade.gov/\n",
    "national-travel-and-tourism-office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "immigration_df = spark.read.parquet('data/i94-sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Temperature Data\n",
    "This dataset was obtained from Kaggle and it explores global temperatures since 1750.\n",
    "\n",
    "The original dataset (in CSV format) can be found [here](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data). The file was converted to parquet as the .parquet file is smaller in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# temperature data\n",
    "temperature_df = spark.read.parquet('data/GlobalLandTemperaturesByCity.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Port of Entry\n",
    "This dataset contains the 3 letter codes used by Customs and Border Protection to indicate ports-of-entry and is used in the i94 immigration data (i94 port field). It is a combination of data obtained [here](https://fam.state.gov/fam/09FAM/09FAM010205.html) and in file I94_SAS_Labels_Descriptions.SAS (available in Udacity's capstone project template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports_of_entry_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=';'\n",
    ").csv('data/port_of_entry_codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Codes\n",
    "This dataset contains country codes used in i94 forms and their respective names. It was also obtained from I94_SAS_Labels_Descriptions.SAS (available in Udacity's capstone project template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=';'\n",
    ").csv('data/country_codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visa Types\n",
    "This dataset contains types of visa obtained from the Department of State - Bureau of Consular Affairs. The information used to build this dataset can be found [here](https://travel.state.gov/content/travel/en/us-visas/visa-information-resources/all-visa-categories.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_types_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=','\n",
    ").csv('data/visa_types.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+--------------------+\n",
      "|type| purpose|code|         description|\n",
      "+----+--------+----+--------------------+\n",
      "|  B1|Business|   1|Visa Holder: Non-...|\n",
      "|  WB|Business|   1|Visa Waiver Progr...|\n",
      "|  GB|Business|   1|Visa Waiver Progr...|\n",
      "| GMB|Business|   1|Guam Marianas Bus...|\n",
      "|   I|Business|   1|Visa Holder: Fore...|\n",
      "+----+--------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_types_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airlines\n",
    "This dataset contains information about almost six thousand airlines and can be found on [Kaggle](https://www.kaggle.com/datasets/open-flights/airline-database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=','\n",
    ").csv('data/airlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "Below are some SQL commands executed to explore the datasets used in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating views to explore the data\n",
    "immigration_df.createOrReplaceTempView('immigration_data')\n",
    "country_codes_df.createOrReplaceTempView('country_codes')\n",
    "airlines_df.createOrReplaceTempView('airlines')\n",
    "ports_of_entry_df.createOrReplaceTempView('ports')\n",
    "visa_types_df.createOrReplaceTempView('visa')\n",
    "\n",
    "epoch = datetime.datetime(1960, 1, 1).date()\n",
    "spark.udf.register(\n",
    "    'isoformat',\n",
    "    lambda x: (epoch + datetime.timedelta(x)).isoformat() if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>port</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>port_city</th>\n",
       "      <th>port_state</th>\n",
       "      <th>visatype</th>\n",
       "      <th>purpose</th>\n",
       "      <th>country</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>QF</td>\n",
       "      <td>Qantas</td>\n",
       "      <td>F</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>VA</td>\n",
       "      <td>Virgin Australia</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748518</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>VA</td>\n",
       "      <td>Viasa</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748518</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>VA</td>\n",
       "      <td>V Australia Airlines</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748519</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>DL</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748520</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>DL</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5748521</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>DL</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5748522</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>HONOLULU</td>\n",
       "      <td>HI</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>M</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5748523</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>HONOLULU</td>\n",
       "      <td>HI</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>F</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5748524</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>HONOLULU</td>\n",
       "      <td>HI</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>F</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id port  arrival_date  departure_date    port_city port_state  \\\n",
       "0  5748517  LOS       20574.0         20582.0  LOS ANGELES         CA   \n",
       "1  5748518  LOS       20574.0         20591.0  LOS ANGELES         CA   \n",
       "2  5748518  LOS       20574.0         20591.0  LOS ANGELES         CA   \n",
       "3  5748518  LOS       20574.0         20591.0  LOS ANGELES         CA   \n",
       "4  5748519  LOS       20574.0         20582.0  LOS ANGELES         CA   \n",
       "5  5748520  LOS       20574.0         20588.0  LOS ANGELES         CA   \n",
       "6  5748521  LOS       20574.0         20588.0  LOS ANGELES         CA   \n",
       "7  5748522  HHW       20574.0         20579.0     HONOLULU         HI   \n",
       "8  5748523  HHW       20574.0         20586.0     HONOLULU         HI   \n",
       "9  5748524  HHW       20574.0         20586.0     HONOLULU         HI   \n",
       "\n",
       "  visatype   purpose      country airline          airline_name gender  age  \n",
       "0       B1  Business    AUSTRALIA      QF                Qantas      F   40  \n",
       "1       B1  Business    AUSTRALIA      VA      Virgin Australia      F   32  \n",
       "2       B1  Business    AUSTRALIA      VA                 Viasa      F   32  \n",
       "3       B1  Business    AUSTRALIA      VA  V Australia Airlines      F   32  \n",
       "4       B1  Business    AUSTRALIA      DL       Delta Air Lines      M   29  \n",
       "5       B1  Business    AUSTRALIA      DL       Delta Air Lines      F   29  \n",
       "6       B1  Business    AUSTRALIA      DL       Delta Air Lines      M   28  \n",
       "7       B2  Pleasure  NEW ZEALAND      NZ       Air New Zealand      M   57  \n",
       "8       B2  Pleasure  NEW ZEALAND      NZ       Air New Zealand      F   66  \n",
       "9       B2  Pleasure  NEW ZEALAND      NZ       Air New Zealand      F   41  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying the data using spark\n",
    "# joining i94 data with the other datasets\n",
    "# filtering out invalid and missing data\n",
    "spark.sql('''\n",
    "    SELECT INT(i.cicid) AS id,\n",
    "           i.i94port AS port,\n",
    "           i.arrdate AS arrival_date,\n",
    "           i.depdate AS departure_date,\n",
    "           SPLIT(p.location, ',')[0] AS port_city,\n",
    "           SPLIT(p.location, ',')[1] AS port_state,\n",
    "           i.visatype,\n",
    "           v.purpose,\n",
    "           cc.country,\n",
    "           i.airline,\n",
    "           a.name AS airline_name,\n",
    "           i.gender,\n",
    "           INT(i.i94bir) AS age\n",
    "    FROM immigration_data i\n",
    "    JOIN country_codes cc ON INT(i.i94res) = cc.code\n",
    "    JOIN airlines a ON i.airline = a.IATA\n",
    "    JOIN ports p ON p.code = i.i94port\n",
    "    LEFT JOIN visa v ON v.type = i.visatype\n",
    "    WHERE cc.country NOT LIKE 'INVALID:%'\n",
    "    AND cc.country NOT LIKE 'No Country Code%'\n",
    "    AND i.gender IS NOT NULL\n",
    "    LIMIT 10\n",
    "''').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visatype</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WT</td>\n",
       "      <td>1309059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2</td>\n",
       "      <td>1117897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WB</td>\n",
       "      <td>282983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>212410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMT</td>\n",
       "      <td>89133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1</td>\n",
       "      <td>39016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E2</td>\n",
       "      <td>19383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CP</td>\n",
       "      <td>14758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E1</td>\n",
       "      <td>3743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>3176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F2</td>\n",
       "      <td>2984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M1</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I1</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GMB</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M2</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SBP</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CPL</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visatype  count(1)\n",
       "0        WT   1309059\n",
       "1        B2   1117897\n",
       "2        WB    282983\n",
       "3        B1    212410\n",
       "4       GMT     89133\n",
       "5        F1     39016\n",
       "6        E2     19383\n",
       "7        CP     14758\n",
       "8        E1      3743\n",
       "9         I      3176\n",
       "10       F2      2984\n",
       "11       M1      1317\n",
       "12       I1       234\n",
       "13      GMB       150\n",
       "14       M2        49\n",
       "15      SBP        11\n",
       "16      CPL        10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common visa types\n",
    "spark.sql('''\n",
    "    SELECT i.visatype, count(*)\n",
    "    FROM immigration_data i\n",
    "    GROUP BY i.visatype\n",
    "    ORDER BY 2 DESC\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the number of distinct ports in immigration data\n",
    "spark.sql('''\n",
    "    SELECT DISTINCT i94port\n",
    "    FROM immigration_data\n",
    "''').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many of these ports exist in port of entry codes\n",
    "spark.sql('''\n",
    "    SELECT code\n",
    "    FROM ports\n",
    "    WHERE code IN (SELECT DISTINCT i94port\n",
    "                   FROM immigration_data)\n",
    "''').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 3096313\n",
      "0 rows removed\n",
      "row count: 3096313\n",
      "+---------+-------+-------+-------+-------+------+-------+-----+------+------+-------+--------------+-----+--------+-------+\n",
      "|    cicid|i94port|arrdate|i94mode|depdate|i94bir|i94visa|occup|gender|insnum|airline|        admnum|fltno|visatype|country|\n",
      "+---------+-------+-------+-------+-------+------+-------+-----+------+------+-------+--------------+-----+--------+-------+\n",
      "|5748517.0|    LOS|20574.0|    1.0|20582.0|  40.0|    1.0| null|     F|  null|     QF|9.495387003E10|00011|      B1|    438|\n",
      "|5748518.0|    LOS|20574.0|    1.0|20591.0|  32.0|    1.0| null|     F|  null|     VA|9.495562283E10|00007|      B1|    438|\n",
      "|5748519.0|    LOS|20574.0|    1.0|20582.0|  29.0|    1.0| null|     M|  null|     DL|9.495640653E10|00040|      B1|    438|\n",
      "|5748520.0|    LOS|20574.0|    1.0|20588.0|  29.0|    1.0| null|     F|  null|     DL|9.495645143E10|00040|      B1|    438|\n",
      "|5748521.0|    LOS|20574.0|    1.0|20588.0|  28.0|    1.0| null|     M|  null|     DL|9.495638813E10|00040|      B1|    438|\n",
      "+---------+-------+-------+-------+-------+------+-------+-----+------+------+-------+--------------+-----+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks\n",
    "# Immigration data\n",
    "def clean_immigration_data(\n",
    "    immigration_df,\n",
    "    country_codes_df,\n",
    "    remove_cols=None,\n",
    "    drop_duplicates=False\n",
    "):\n",
    "    '''\n",
    "    Clean Immigration data by removing invalid and not given country codes\n",
    "    Args:\n",
    "        remove_cols (list): list of columns to remove from the dataframe\n",
    "        drop_duplicates (bool): allow dropping duplicate records\n",
    "    Returns:\n",
    "        Immigration data dataframe\n",
    "    '''\n",
    "\n",
    "    immigration_df.createOrReplaceTempView('immigration_data')\n",
    "    country_codes_df.createOrReplaceTempView('country_codes')\n",
    "\n",
    "    pre_clean_rows = immigration_df.count()\n",
    "    print(f'row count: {pre_clean_rows}')\n",
    "\n",
    "    immigration_df = spark.sql('''\n",
    "        SELECT i.*, INT(i.i94res) AS country\n",
    "        FROM immigration_data i\n",
    "        JOIN country_codes cc ON INT(i.i94res) = cc.code\n",
    "        WHERE cc.country NOT LIKE 'INVALID:%'\n",
    "        AND cc.country NOT LIKE 'No Country Code%'\n",
    "    ''')\n",
    "\n",
    "    rows_removed = immigration_df.count() - pre_clean_rows\n",
    "\n",
    "    print(f'{rows_removed} rows removed')\n",
    "\n",
    "    if drop_duplicates:\n",
    "        # Removing duplicate records\n",
    "        immigration_df.dropDuplicates()\n",
    "\n",
    "    # removing unnecessary columns\n",
    "    if remove_cols:\n",
    "        immigration_df = immigration_df.drop(*remove_cols)\n",
    "\n",
    "    print(f'row count: {immigration_df.count()}')\n",
    "\n",
    "    return immigration_df\n",
    "\n",
    "\n",
    "cols = [\n",
    "    'biryear', 'count', 'dtaddto', 'dtadfile',\n",
    "    'entdepa', 'entdepd', 'entdepu',\n",
    "    'i94yr', 'i94mon', 'i94cit', 'i94addr', 'i94res',\n",
    "    'matflag', 'visapost',\n",
    "]\n",
    "\n",
    "# calling the method to remove duplicates and drop unnecessary columns\n",
    "immigration_df = clean_immigration_data(\n",
    "    immigration_df, country_codes_df,\n",
    "    remove_cols=cols, drop_duplicates=True)\n",
    "\n",
    "immigration_df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 15:28:18 WARN SimpleFunctionRegistry: The function isoformat replaced a previously registered function.\n",
      "row count: 3096313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+------------+--------------+----+----+--------+-------+---+------+----------+--------------+----------+\n",
      "|     id|port|country|arrival_date|departure_date|mode|visa|visatype|airline|age|gender|flight_num| admission_num|occupation|\n",
      "+-------+----+-------+------------+--------------+----+----+--------+-------+---+------+----------+--------------+----------+\n",
      "|5748517| LOS|    438|  2016-04-30|    2016-05-08| 1.0| 1.0|      B1|     QF| 40|     F|     00011|9.495387003E10|      null|\n",
      "|5748518| LOS|    438|  2016-04-30|    2016-05-17| 1.0| 1.0|      B1|     VA| 32|     F|     00007|9.495562283E10|      null|\n",
      "|5748519| LOS|    438|  2016-04-30|    2016-05-08| 1.0| 1.0|      B1|     DL| 29|     M|     00040|9.495640653E10|      null|\n",
      "|5748520| LOS|    438|  2016-04-30|    2016-05-14| 1.0| 1.0|      B1|     DL| 29|     F|     00040|9.495645143E10|      null|\n",
      "|5748521| LOS|    438|  2016-04-30|    2016-05-14| 1.0| 1.0|      B1|     DL| 28|     M|     00040|9.495638813E10|      null|\n",
      "+-------+----+-------+------------+--------------+----+----+--------+-------+---+------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def transform_immigration_data(df):\n",
    "    '''\n",
    "    renames columns, cast data types and formats values in immigration_data\n",
    "    Args:\n",
    "        df (object): spark dataframe\n",
    "    Returns:\n",
    "        immigration data dataframe\n",
    "    '''\n",
    "\n",
    "    df.createOrReplaceTempView('immigration_data')\n",
    "    epoch = datetime.datetime(1960, 1, 1).date()\n",
    "\n",
    "    spark.udf.register(\n",
    "        'isoformat',\n",
    "        lambda x: (epoch + datetime.timedelta(x)).isoformat() if x else None\n",
    "    )\n",
    "\n",
    "    df = spark.sql('''\n",
    "        SELECT INT(i.cicid) AS id,\n",
    "            i.i94port AS port,\n",
    "            i.country AS country,\n",
    "            isoformat(int(i.arrdate)) AS arrival_date,\n",
    "            isoformat(int(i.depdate)) AS departure_date,\n",
    "            i.i94mode AS mode,\n",
    "            i.i94visa AS visa,\n",
    "            i.visatype,\n",
    "            i.airline,\n",
    "            INT(i.i94bir) AS age,\n",
    "            i.gender,\n",
    "            i.fltno AS flight_num,\n",
    "            i.admnum AS admission_num,\n",
    "            i.occup AS occupation\n",
    "        FROM immigration_data i\n",
    "    ''')\n",
    "\n",
    "    print(f'row count: {df.count()}')\n",
    "    return df\n",
    "\n",
    "immigration_df = transform_immigration_data(immigration_df)\n",
    "immigration_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+------------------+\n",
      "|   city|      country|month|          avg_temp|\n",
      "+-------+-------------+-----+------------------+\n",
      "|Abilene|United States|    1| 5.310202072538861|\n",
      "|Abilene|United States|    2| 7.527417525773191|\n",
      "|Abilene|United States|    3| 12.06452577319588|\n",
      "|Abilene|United States|    4| 16.99996907216495|\n",
      "|Abilene|United States|    5|21.741757731958753|\n",
      "+-------+-------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Temperature data\n",
    "\n",
    "def create_avg_temperature_df(df, country=None):\n",
    "    '''\n",
    "    create a dataframe with the average temperatures per country\n",
    "    if country is provided, then it returns data for just that country.\n",
    "    Args:\n",
    "        df (object): spark dataframe\n",
    "        country (str): country name to use for calculations\n",
    "    Returns:\n",
    "        average temperature dataframe\n",
    "    '''\n",
    "\n",
    "    df.createOrReplaceTempView('temperature')\n",
    "\n",
    "    sql = '''\n",
    "        SELECT city, country, MONTH(dt) month, AVG(averagetemperature) avg_temp\n",
    "        FROM temperature\n",
    "        WHERE averagetemperature IS NOT NULL\n",
    "    '''\n",
    "    if country:\n",
    "        sql += f'''    AND LOWER(country) = '{country}' '''\n",
    "\n",
    "    sql += '''\n",
    "        GROUP BY city, country, month\n",
    "        ORDER BY city, country, month\n",
    "    '''\n",
    "    return spark.sql(sql)\n",
    "\n",
    "\n",
    "# average temperature of cities in the US\n",
    "temperature_df = create_avg_temperature_df(\n",
    "    temperature_df, country='united states')\n",
    "\n",
    "temperature_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------+\n",
      "|code|       city|   state|\n",
      "+----+-----------+--------+\n",
      "| ANC|  ANCHORAGE|      AK|\n",
      "| MOB|     MOBILE|      AL|\n",
      "| LIA|LITTLE ROCK|AR (BPS)|\n",
      "| NOG|    NOGALES|      AZ|\n",
      "| PHO|    PHOENIX|      AZ|\n",
      "+----+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_clean_ports(country):\n",
    "    '''\n",
    "    removes ports where state is null\n",
    "    and country is not equal to the country provided\n",
    "    Args:\n",
    "        country (str): country name to filter the records\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    clean_ports = spark.sql(f'''\n",
    "        SELECT code,\n",
    "            SPLIT(location, ',')[0] city,\n",
    "            TRIM(SPLIT(location, ',')[1]) state\n",
    "        FROM ports\n",
    "        WHERE TRIM(SPLIT(location, ',')[1]) IS NOT NULL\n",
    "        AND LOWER( SPLIT(location, ',')[0]) IN (SELECT DISTINCT LOWER(city)\n",
    "                                                FROM temperature\n",
    "                                                WHERE LOWER(country) = '{country}')\n",
    "        ''')\n",
    "\n",
    "    clean_ports.createOrReplaceTempView('clean_ports')\n",
    "    return clean_ports\n",
    "\n",
    "\n",
    "clean_ports = create_clean_ports(country='united states')\n",
    "clean_ports.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Royal Tongan Airlines', code='WR', country='Tonga'),\n",
       " Row(name='Shuttle America', code='S5', country='United States'),\n",
       " Row(name='Trast Aero', code='S5', country='Kyrgyzstan'),\n",
       " Row(name='Virgin America', code='VX', country='United States'),\n",
       " Row(name='WestJet Encore', code='WR', country='Canada')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Airlines data\n",
    "\n",
    "def clean_airlines(df):\n",
    "    '''\n",
    "    Filters out invalid rows, renames columns\n",
    "    Selects only airlines in i94 immigration data\n",
    "    Args:\n",
    "        df (object): spark dataframe\n",
    "    Returns:\n",
    "        airlines dataframe\n",
    "    '''\n",
    "\n",
    "    df.createOrReplaceTempView('airlines')\n",
    "\n",
    "    df = spark.sql('''\n",
    "        SELECT a.name,\n",
    "            a.iata code,\n",
    "            a.country\n",
    "        FROM airlines a\n",
    "        WHERE a.iata IS NOT NULL\n",
    "        AND a.iata IN (SELECT DISTINCT airline\n",
    "                    FROM immigration_data)\n",
    "        AND a.iata <> '-'\n",
    "    ''')\n",
    "\n",
    "    print(f'row count: {df.count()}')\n",
    "    return df\n",
    "\n",
    "\n",
    "airlines_df = clean_airlines(airlines_df)\n",
    "airlines_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step3'></a>\n",
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The data model was designed using star schema, i.e., it contains a fact table and dimension table. It was chosen as it is more performant for data analysis and reporting tools than the traditional relational model.\n",
    "\n",
    "![Data Model diagram](img/database_diagram.png)\n",
    "\n",
    "##### Fact\n",
    "`arrivals` - data from i94 immigration data\n",
    " - id\n",
    " - airport\n",
    " - arrival_date\n",
    " - departure_date\n",
    " - mode\n",
    " - visa\n",
    " - visatype\n",
    " - age\n",
    " - gender\n",
    " - airline\n",
    " - flight_num\n",
    " - occupation\n",
    " - admission_num\n",
    " - country\n",
    "\n",
    "##### Dimensions \n",
    "`visa` - visa types\n",
    "  - type\n",
    "  - purpose\n",
    "  - code\n",
    "  - description\n",
    "\n",
    "\n",
    "`arrival_mode` - arrival mode\n",
    "  - code\n",
    "  - desc\n",
    "\n",
    "\n",
    "`calendar` - date dimension\n",
    "  - date\n",
    "  - day\n",
    "  - week_day\n",
    "  - month\n",
    "  - month_name\n",
    "  - quarter\n",
    "  - year\n",
    "  - season\n",
    "  - season_name\n",
    "\n",
    "\n",
    "`ports` - ports of entry codes\n",
    "  - code\n",
    "  - city\n",
    "  - state\n",
    "\n",
    "\n",
    "`airlines` - airlines information\n",
    "  - name\n",
    "  - code\n",
    "  - country\n",
    "\n",
    "\n",
    "`countries` - countries dimension\n",
    "  - code\n",
    "  - country\n",
    "\n",
    "\n",
    "`temperatures` - average temperatures of cities in the US\n",
    "  - city\n",
    "  - month\n",
    "  - avg_temp \n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "- Extract the data from the various data sources and load them into pySpark dataframes\n",
    "- Transform the data using pySpark.sql\n",
    "- Load the data into the proposed star schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step4'></a>\n",
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "- This step was already done in previous steps:\n",
    "    - Data is already loaded into dataframes\n",
    "    - Data was already transformed using pySpark.sql\n",
    "    - `calendar` and `arrival_mode` still need to be created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(date=datetime.date(2000, 1, 1), day=1, week_day='Saturday', month=1, month_name='January', quarter=1, year=2000, season=1, season_name='winter'),\n",
       " Row(date=datetime.date(2000, 1, 2), day=2, week_day='Sunday', month=1, month_name='January', quarter=1, year=2000, season=1, season_name='winter'),\n",
       " Row(date=datetime.date(2000, 1, 3), day=3, week_day='Monday', month=1, month_name='January', quarter=1, year=2000, season=1, season_name='winter'),\n",
       " Row(date=datetime.date(2000, 1, 4), day=4, week_day='Tuesday', month=1, month_name='January', quarter=1, year=2000, season=1, season_name='winter'),\n",
       " Row(date=datetime.date(2000, 1, 5), day=5, week_day='Wednesday', month=1, month_name='January', quarter=1, year=2000, season=1, season_name='winter')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_date_dimension(start_date, end_date):\n",
    "    '''\n",
    "    Creates the date dimension (calendar)\n",
    "    Args:\n",
    "        start_date (datetime): the start date of the date dimension\n",
    "        end_date (datetime): the end date of the date dimension\n",
    "    Returns:\n",
    "        calendar schema\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'date': pd.date_range(start_date, end_date)\n",
    "    })\n",
    "\n",
    "    df['day'] = df.date.dt.day\n",
    "    df['week_day'] = df.date.dt.day_name()\n",
    "    df['month'] = df.date.dt.month\n",
    "    df['month_name'] = df.date.dt.month_name()\n",
    "    df['quarter'] = df.date.dt.quarter\n",
    "    df['year'] = df.date.dt.year\n",
    "\n",
    "    df['season'] = df.date.dt.month % 12 // 3 + 1\n",
    "    df['season_name'] = df['season'].map(\n",
    "        {1: 'winter', 2: 'spring', 3: 'summer', 4: 'fall'})\n",
    "\n",
    "    calendar_schema = StructType([\n",
    "        StructField('date', DateType(), True),\n",
    "        StructField('day', IntegerType(), True),\n",
    "        StructField('week_day', StringType(), True),\n",
    "        StructField('month', IntegerType(), True),\n",
    "        StructField('month_name', StringType(), True),\n",
    "        StructField('quarter', IntegerType(), True),\n",
    "        StructField('year', IntegerType(), True),\n",
    "        StructField('season', IntegerType(), True),\n",
    "        StructField('season_name', StringType(), True)\n",
    "    ])\n",
    "\n",
    "    calendar_df = spark.createDataFrame(df, schema=calendar_schema)\n",
    "    return calendar_df\n",
    "\n",
    "calendar_df = create_date_dimension('2000-01-01', '2030-12-31')\n",
    "calendar_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrival_mode dimension\n",
    "def create_arrival_mode_dimension(data, columns):\n",
    "    '''\n",
    "    Creates the arrival mode dimension\n",
    "    Args:\n",
    "        data (list): Values to generate the RDD table\n",
    "        columns (list): List of column names\n",
    "    '''\n",
    "    # create RDD\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    arrival_mode_df = rdd.toDF(columns)\n",
    "    return arrival_mode_df\n",
    "\n",
    "\n",
    "data = [(1, 'Air'), (2, 'Sea'), (3, 'Land'), (9, 'Not Reported')]\n",
    "columns = ['code', 'description']\n",
    "arrival_mode_df = create_arrival_mode_dimension(data, columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we generate parquet files based on the transformed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def generate_parquet_files(output_folder):\n",
    "    '''\n",
    "    Generate parquet files for the star schema\n",
    "    Overwrites existing files\n",
    "    Args:\n",
    "        output_folder (str): destination folder for all parquet files\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # arrivals\n",
    "    immigration_df.repartition(40).write.parquet(f'{output_folder}/arrivals/', mode='overwrite')\n",
    "\n",
    "    # visa\n",
    "    visa_types_df.write.parquet(f'{output_folder}/visa/', mode='overwrite')\n",
    "\n",
    "    # arrival_mode\n",
    "    arrival_mode_df.write.parquet(f'{output_folder}/arrival_mode/', mode='overwrite')\n",
    "\n",
    "    # calendar\n",
    "    calendar_df.write.parquet(f'{output_folder}/calendar/', mode='overwrite')\n",
    "\n",
    "    # ports\n",
    "    clean_ports.write.parquet(f'{output_folder}/ports/', mode='overwrite')\n",
    "\n",
    "    # airlines\n",
    "    airlines_df.write.parquet(f'{output_folder}/airlines/', mode='overwrite')\n",
    "\n",
    "    # countries\n",
    "    country_codes_df.write.parquet(f'{output_folder}/countries/', mode='overwrite')\n",
    "\n",
    "    # temperatures\n",
    "    temperature_df.write.parquet(f'{output_folder}/temperatures/', mode='overwrite')\n",
    "\n",
    "\n",
    "generate_parquet_files('data/output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Verify if dataframes have data after previous steps were executed\n",
    " * Verify if parquet files were created correctly and are in the right directory\n",
    " * Compare row count of parquet files and dataframes\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No empty dataframes!\n",
      "All tables were created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parquet files passed the check!\n",
      "All quality checks passed!\n"
     ]
    }
   ],
   "source": [
    "import os.path as path\n",
    "\n",
    "# all parquet folders created\n",
    "tables = [\n",
    "    'arrivals', 'visa', 'arrival_mode',\n",
    "    'ports', 'temperatures', 'airlines',\n",
    "    'countries', 'calendar'\n",
    "]\n",
    "# all dataframes created\n",
    "dataframes = [\n",
    "    immigration_df, visa_types_df, arrival_mode_df,\n",
    "    clean_ports, temperature_df, airlines_df,\n",
    "    country_codes_df, calendar_df\n",
    "]\n",
    "\n",
    "\n",
    "def check_dataframes(dataframes):\n",
    "    '''\n",
    "    Checks if all dataframes contain records\n",
    "    Args:\n",
    "        dataframes (list): list with all dataframes that should be checked\n",
    "    Returns:\n",
    "        boolean - True if all dataframes contain records\n",
    "    '''\n",
    "    dataframes_ok = True\n",
    "    for df in dataframes:\n",
    "        if df.count() < 1:\n",
    "            dataframes_ok = False\n",
    "            print(f'dataframe {df} is empty')\n",
    "    if dataframes_ok:\n",
    "        print('No empty dataframes!')\n",
    "    return dataframes_ok\n",
    "\n",
    "\n",
    "def check_tables(tables, output_folder='data/output'):\n",
    "    '''\n",
    "    Verify if parquet files were created correctly\n",
    "    Args:\n",
    "        tables (list): list with all tables that should be checked\n",
    "    Returns:\n",
    "        boolean - True if all tables were created\n",
    "    '''\n",
    "    tables_ok = True\n",
    "    for table in tables:\n",
    "        if not path.isfile(f'{output_folder}/{table}/_SUCCESS'):\n",
    "            tables_ok = False\n",
    "            print(f'Table {table} was NOT created successfuly')\n",
    "    if tables_ok:\n",
    "        print('All tables were created successfully!')\n",
    "    return tables_ok\n",
    "\n",
    "\n",
    "def check_rows(tables, dataframes, output_folder='data/output'):\n",
    "    '''\n",
    "    Verify if row count of parquet files and dataframes are equal\n",
    "    Args:\n",
    "        tables (list): list with all tables that should be checked\n",
    "        dataframes (list): list with all dataframes that should be checked\n",
    "        output_folder (str, optional): folder where the output files should be\n",
    "    Returns:\n",
    "        boolean - True if all checks passed\n",
    "    '''\n",
    "    rows_ok = True\n",
    "\n",
    "    # combine tables and dataframes to check both at the same time\n",
    "    tables = list(zip(tables, dataframes))\n",
    "\n",
    "    for table, df in tables:\n",
    "        parquet_rows = spark.read.parquet(\n",
    "            f'{output_folder}/{table}').count()\n",
    "        if parquet_rows != df.count():\n",
    "            rows_ok = False\n",
    "            print(f'Row count for {table} does not match with the number of rows in the dataframe')\n",
    "\n",
    "    if rows_ok:\n",
    "        print('All parquet files passed the check!')\n",
    "\n",
    "    return rows_ok\n",
    "\n",
    "\n",
    "# Perform all checks\n",
    "dataframes_ok = check_dataframes(dataframes)\n",
    "tables_ok = check_tables(tables)\n",
    "rows_ok = check_rows(tables, dataframes)\n",
    "\n",
    "if dataframes_ok and tables_ok and rows_ok:\n",
    "    print('All quality checks passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrivals\n",
    "- id: integer (PK) - arrival unique id\n",
    "- port - string (FK) - references port.code\n",
    "- arrival_date - string (FK)- references calendar.date\n",
    "- departure_date - string (FK) - references calendar.date\n",
    "- mode - string (FK) - references arrival_mode.code\n",
    "- visa - string\n",
    "- visatype - string - references visa.type\n",
    "- age - integer - immigrant's age\n",
    "- gender - string - immigrant's gender\n",
    "- airline - string (FK) - references airline.code\n",
    "- flight_num - string - flight number\n",
    "- occupation - string - immigrant's occupation\n",
    "- admission_num - string\n",
    "- country - integer (FK) - references countries.code\n",
    "\n",
    "### Visa\n",
    "- type - string (PK) - visa type (WT, B1, ...)\n",
    "- purpose - visa purpose (Business, Pleasure, etc.)\n",
    "- code - integer - visa i94 code\n",
    "- description - string\n",
    "\n",
    "### Arrival_mode\n",
    "- code - long (PK) - arrival mode code\n",
    "- description - string\n",
    "\n",
    "### Calendar\n",
    "- date - date (PK) - date in the format YYYY-MM-DD\n",
    "- day - integer\n",
    "- week_day - string\n",
    "- month - integer\n",
    "- month_name - string\n",
    "- quarter - integer\n",
    "- year - integer\n",
    "- season - integer\n",
    "- season_name - string\n",
    "\n",
    "### Ports\n",
    "- code - string (PK) - port of entry code\n",
    "- city - string (FK) - references temperatures.city\n",
    "- state - string\n",
    "\n",
    "### Airlines\n",
    "- code - string (PK) - IATA code\n",
    "- name - string - airline company name\n",
    "- country - string - country of origin\n",
    "\n",
    "### Countries\n",
    "- code - integer (PK)\n",
    "- country - string\n",
    "\n",
    "### Temperatures\n",
    "- city - integer (PK)\n",
    "- month - integer\n",
    "- avg_temp - double - historical average temperature per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step5'></a>\n",
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "    - Python 3.8 was chosen as it is a stable version of one of the most used programming languages. Python is very popularly used in data-driven projects as it contains a vast amount of libraries and functions designed for this purpose.\n",
    "\n",
    "    - PySpark was chosen because it provides tools to explore the data in a scalable manner, allowing developers to use SQL to process data in combination with Python.\n",
    "    It also performs better than other Python libraries given that it relies on Apache Spark to do the heavy lifting.\n",
    "\n",
    "    - Parquet files were chosen because they are a good way to compress data and can easily be used by other tools like pandas/pyarrow and robust databases.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "    - The data should be updated everytime a new dataset is released by the US National Tourism Trade Office. Ideally, the data would be updated daily, but given that processing this amount of data may not be at the top of the government's priority and compiling this data must be a laborious process, a monthly if not a biweekly update could already give decision makers some insight and allow them to follow trends with minimum delay.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "    - In this case, ETL scripts could be used to run on a Hadoop cluster. Amazon EMR could be leveraged to avoid the need to handle processing on-premises and S3 buckets could be used to store the parquet files and other related resources. \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - For this scenario, Apache Airflow could be used to automate the workflow and orchestrate the runs and the data processing.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "    - Amazon Redshift could be used to handle all simultaneous connections as it is a stable and reliable database management system and it would still be a part of the same suite of products (AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step6'></a>\n",
    "#### Step 6: Data Analysis Demo over the processed tables\n",
    "\n",
    "Below are some examples of analyses that can be performed over the tables generated by the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all parquet files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrivals_df     = spark.read.parquet('data/output/arrivals/')\n",
    "visa_df         = spark.read.parquet('data/output/visa/')\n",
    "arrival_mode_df = spark.read.parquet('data/output/arrival_mode/')\n",
    "calendar_df     = spark.read.parquet('data/output/calendar/')\n",
    "ports_df        = spark.read.parquet('data/output/ports/')\n",
    "airlines_df     = spark.read.parquet('data/output/airlines/')\n",
    "countries_df    = spark.read.parquet('data/output/countries/')\n",
    "temperatures_df = spark.read.parquet('data/output/temperatures/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataframes into Spark so we can create SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrivals_df.createOrReplaceTempView('arrivals')\n",
    "visa_df.createOrReplaceTempView('visa')\n",
    "arrival_mode_df.createOrReplaceTempView('arrival_mode')\n",
    "calendar_df.createOrReplaceTempView('calendar')\n",
    "ports_df.createOrReplaceTempView('ports')\n",
    "airlines_df.createOrReplaceTempView('airlines')\n",
    "countries_df.createOrReplaceTempView('countries')\n",
    "temperatures_df.createOrReplaceTempView('temperatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>all_time_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miramar</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coral Springs</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miami</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pembroke Pines</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hialeah</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Port Saint Lucie</td>\n",
       "      <td>23.061794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cape Coral</td>\n",
       "      <td>23.025512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Brownsville</td>\n",
       "      <td>22.614672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city  all_time_avg\n",
       "0           Miramar     23.061794\n",
       "1   Fort Lauderdale     23.061794\n",
       "2     Coral Springs     23.061794\n",
       "3             Miami     23.061794\n",
       "4    Pembroke Pines     23.061794\n",
       "5           Hialeah     23.061794\n",
       "6         Hollywood     23.061794\n",
       "7  Port Saint Lucie     23.061794\n",
       "8        Cape Coral     23.025512\n",
       "9       Brownsville     22.614672"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing top 10 cities with the highest temperatures\n",
    "# Calculating city averages\n",
    "# Sorting average temperature in descending order\n",
    "spark.sql('''\n",
    "    SELECT city, AVG(avg_temp) AS all_time_avg\n",
    "    FROM temperatures\n",
    "    GROUP BY city\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 10\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>immigrants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>M</td>\n",
       "      <td>155449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>F</td>\n",
       "      <td>136026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAPAN</td>\n",
       "      <td>M</td>\n",
       "      <td>106831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAPAN</td>\n",
       "      <td>F</td>\n",
       "      <td>101975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEXICO</td>\n",
       "      <td>M</td>\n",
       "      <td>90329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHINA, PRC</td>\n",
       "      <td>F</td>\n",
       "      <td>87019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MEXICO</td>\n",
       "      <td>F</td>\n",
       "      <td>86778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHINA, PRC</td>\n",
       "      <td>M</td>\n",
       "      <td>81585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FRANCE</td>\n",
       "      <td>M</td>\n",
       "      <td>77932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FRANCE</td>\n",
       "      <td>F</td>\n",
       "      <td>77570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          country gender  immigrants\n",
       "0  UNITED KINGDOM      M      155449\n",
       "1  UNITED KINGDOM      F      136026\n",
       "2           JAPAN      M      106831\n",
       "3           JAPAN      F      101975\n",
       "4          MEXICO      M       90329\n",
       "5      CHINA, PRC      F       87019\n",
       "6          MEXICO      F       86778\n",
       "7      CHINA, PRC      M       81585\n",
       "8          FRANCE      M       77932\n",
       "9          FRANCE      F       77570"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discovering where immigrants are coming from\n",
    "# Aggregating data by country of origin and gender\n",
    "# Showing only the top 10 results\n",
    "spark.sql('''\n",
    "    SELECT c.country, a.gender, COUNT(*) AS immigrants\n",
    "    FROM arrivals a\n",
    "    JOIN countries AS c ON a.country = c.code\n",
    "    GROUP BY c.country, a.gender\n",
    "    ORDER BY immigrants DESC, c.country\n",
    "    LIMIT 10\n",
    "''').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>immigrants</th>\n",
       "      <th>state</th>\n",
       "      <th>week_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5797</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5085</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5485</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6645</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3955</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>8567</td>\n",
       "      <td>WA</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>9616</td>\n",
       "      <td>WA</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>6442</td>\n",
       "      <td>WA</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>7347</td>\n",
       "      <td>WA</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>5360</td>\n",
       "      <td>WA</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     immigrants state   week_day\n",
       "0          5797    AZ  Wednesday\n",
       "1          5085    AZ    Tuesday\n",
       "2          5485    AZ   Thursday\n",
       "3          6645    AZ   Saturday\n",
       "4          3955    AZ     Sunday\n",
       "..          ...   ...        ...\n",
       "150        8567    WA     Friday\n",
       "151        9616    WA   Saturday\n",
       "152        6442    WA     Monday\n",
       "153        7347    WA     Sunday\n",
       "154        5360    WA   Thursday\n",
       "\n",
       "[155 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of immigrant arrivals per day of the week per state\n",
    "# Filtering out results with less than 100 arrivals\n",
    "# Sorting the results per state\n",
    "spark.sql('''\n",
    "    SELECT COUNT(a.id) immigrants, p.state, c.week_day\n",
    "    FROM arrivals a\n",
    "    JOIN calendar c ON c.date = a.arrival_date\n",
    "    JOIN ports p ON p.code = a.port\n",
    "    GROUP BY p.state, c.week_day\n",
    "    HAVING immigrants > 100\n",
    "    ORDER BY p.state\n",
    "''').toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('capstone_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6edbed45810cd0e67f981f8c41bf8a7d667105834d95c0a12c67fb49a50aebae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
