{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project is to analyze US immigration and tourism data. To achieve this goal, the data is extracted from different datasets and transformed to work in a star schema. This schema is then ready to be used to observe trends and behaviors in the data over time.\n",
    "\n",
    "The project was developed according to the following steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#step1)\n",
    "* [Step 2: Explore and Assess the Data](#step2)\n",
    "* [Step 3: Define the Data Model](#step3)\n",
    "* [Step 4: Run ETL to Model the Data](#step4)\n",
    "* [Step 5: Complete Project Write Up](#step5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.0') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.executor.memory', '12g') \\\n",
    "    .config('spark.driver.memory', '12g') \\\n",
    "    .config('spark.memory.offHeap.enabled', True) \\\n",
    "    .config('spark.memory.offHeap.size', '12g') \\\n",
    "    .config('spark.sql.shuffle.partitions', 64) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "- Explain what you plan to do in the project in more detail:\n",
    "    - Import data from US Immigration Data, Airlines and Global Temperature datasets\n",
    "    - Load the data into pySpark dataframes to transform the data accordingly\n",
    "    - Create a star schema and populate the fact and dimensions with data from the datasets\n",
    "    - Store the data in parquet files that can be used in a Data Warehouse for data analysis purposes\n",
    "- What data do you use?\n",
    "    - I94 US Immigration Data - 2016\n",
    "    - World Airports Data\n",
    "    - World Tempearture Data\n",
    "- What is your end solution look like?\n",
    "    - Tables stored in a star schema built to perform data analysis on immigration data in the US\n",
    "- What tools did you use?\n",
    "    - Python 3.8 (dependencies listed in requirements.txt)\n",
    "    - Pyspark 3.3.0 (Scala version 2.12.15)\n",
    "    - OpenJDK 64-Bit Server VM (build 11.0.15+10-Ubuntu-0ubuntu0.20.04.1, mixed mode, sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I94 Immigration Data\n",
    "\n",
    "This data was provided by the US National Tourism and Trade Office and contains statistics of visitor arrival during 2016.\n",
    "\n",
    "The original dataset is in SAS7BDAT format and can be found [here](https://www.trade.gov/\n",
    "national-travel-and-tourism-office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "immigration_df = spark.read.parquet('data/i94-sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Temperature Data\n",
    "This dataset was obtained from Kaggle and it explores global temperatures since 1750.\n",
    "\n",
    "The original dataset (in CSV format) can be found [here](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data). The file was converted to parquet as the .parquet file is smaller in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# temperature data\n",
    "temperature_df = spark.read.parquet('data/GlobalLandTemperaturesByCity.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Port of Entry\n",
    "This dataset contains the 3 letter codes used by Customs and Border Protection to indicate ports-of-entry and is used in the i94 immigration data (i94 port field). It is a combination of data obtained [here](https://fam.state.gov/fam/09FAM/09FAM010205.html) and in file I94_SAS_Labels_Descriptions.SAS (available in Udacity's capstone project template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports_of_entry_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=';'\n",
    ").csv('data/port_of_entry_codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Codes\n",
    "This dataset contains country codes used in i94 forms and their respective names. It was also obtained from I94_SAS_Labels_Descriptions.SAS (available in Udacity's capstone project template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=';'\n",
    ").csv('data/country_codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visa Types\n",
    "This dataset contains types of visa obtained from the Department of State - Bureau of Consular Affairs. The information used to build this dataset can be found [here](https://travel.state.gov/content/travel/en/us-visas/visa-information-resources/all-visa-categories.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_types_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=','\n",
    ").csv('data/visa_types.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+--------------------+\n",
      "|type| purpose|code|         description|\n",
      "+----+--------+----+--------------------+\n",
      "|  B1|Business|   1|Visa Holder: Non-...|\n",
      "|  WB|Business|   1|Visa Waiver Progr...|\n",
      "|  GB|Business|   1|Visa Waiver Progr...|\n",
      "| GMB|Business|   1|Guam Marianas Bus...|\n",
      "|   I|Business|   1|Visa Holder: Fore...|\n",
      "+----+--------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_types_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airlines\n",
    "This dataset contains information about almost six thousand airlines and can be found on [Kaggle](https://www.kaggle.com/datasets/open-flights/airline-database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_df = spark.read.options(\n",
    "    header=True, inferSchema=True, delimiter=','\n",
    ").csv('data/airlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "Below are some SQL commands executed to explore the datasets used in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating views to explore the data\n",
    "immigration_df.createOrReplaceTempView('immigration_data')\n",
    "country_codes_df.createOrReplaceTempView('country_codes')\n",
    "airlines_df.createOrReplaceTempView('airlines')\n",
    "ports_of_entry_df.createOrReplaceTempView('ports')\n",
    "visa_types_df.createOrReplaceTempView('visa')\n",
    "\n",
    "epoch = datetime.datetime(1960, 1, 1).date()\n",
    "spark.udf.register(\n",
    "    'isoformat',\n",
    "    lambda x: (epoch + datetime.timedelta(x)).isoformat() if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>port</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>port_city</th>\n",
       "      <th>port_state</th>\n",
       "      <th>visatype</th>\n",
       "      <th>purpose</th>\n",
       "      <th>country</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>QF</td>\n",
       "      <td>Qantas</td>\n",
       "      <td>F</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>VA</td>\n",
       "      <td>Virgin Australia</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748518</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>VA</td>\n",
       "      <td>Viasa</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748518</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>VA</td>\n",
       "      <td>V Australia Airlines</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748519</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>DL</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748520</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>DL</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5748521</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>DL</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5748522</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>HONOLULU</td>\n",
       "      <td>HI</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>M</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5748523</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>HONOLULU</td>\n",
       "      <td>HI</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>F</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5748524</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>HONOLULU</td>\n",
       "      <td>HI</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>F</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id port  arrival_date  departure_date    port_city port_state  \\\n",
       "0  5748517  LOS       20574.0         20582.0  LOS ANGELES         CA   \n",
       "1  5748518  LOS       20574.0         20591.0  LOS ANGELES         CA   \n",
       "2  5748518  LOS       20574.0         20591.0  LOS ANGELES         CA   \n",
       "3  5748518  LOS       20574.0         20591.0  LOS ANGELES         CA   \n",
       "4  5748519  LOS       20574.0         20582.0  LOS ANGELES         CA   \n",
       "5  5748520  LOS       20574.0         20588.0  LOS ANGELES         CA   \n",
       "6  5748521  LOS       20574.0         20588.0  LOS ANGELES         CA   \n",
       "7  5748522  HHW       20574.0         20579.0     HONOLULU         HI   \n",
       "8  5748523  HHW       20574.0         20586.0     HONOLULU         HI   \n",
       "9  5748524  HHW       20574.0         20586.0     HONOLULU         HI   \n",
       "\n",
       "  visatype   purpose      country airline          airline_name gender  age  \n",
       "0       B1  Business    AUSTRALIA      QF                Qantas      F   40  \n",
       "1       B1  Business    AUSTRALIA      VA      Virgin Australia      F   32  \n",
       "2       B1  Business    AUSTRALIA      VA                 Viasa      F   32  \n",
       "3       B1  Business    AUSTRALIA      VA  V Australia Airlines      F   32  \n",
       "4       B1  Business    AUSTRALIA      DL       Delta Air Lines      M   29  \n",
       "5       B1  Business    AUSTRALIA      DL       Delta Air Lines      F   29  \n",
       "6       B1  Business    AUSTRALIA      DL       Delta Air Lines      M   28  \n",
       "7       B2  Pleasure  NEW ZEALAND      NZ       Air New Zealand      M   57  \n",
       "8       B2  Pleasure  NEW ZEALAND      NZ       Air New Zealand      F   66  \n",
       "9       B2  Pleasure  NEW ZEALAND      NZ       Air New Zealand      F   41  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying the data using spark\n",
    "# joining i94 data with the other datasets\n",
    "# filtering out invalid and missing data\n",
    "spark.sql('''\n",
    "    SELECT INT(i.cicid) AS id,\n",
    "           i.i94port AS port,\n",
    "           i.arrdate AS arrival_date,\n",
    "           i.depdate AS departure_date,\n",
    "           SPLIT(p.location, ',')[0] AS port_city,\n",
    "           SPLIT(p.location, ',')[1] AS port_state,\n",
    "           i.visatype,\n",
    "           v.purpose,\n",
    "           cc.country,\n",
    "           i.airline,\n",
    "           a.name AS airline_name,\n",
    "           i.gender,\n",
    "           INT(i.i94bir) AS age\n",
    "    FROM immigration_data i\n",
    "    JOIN country_codes cc ON INT(i.i94res) = cc.code\n",
    "    JOIN airlines a ON i.airline = a.IATA\n",
    "    JOIN ports p ON p.code = i.i94port\n",
    "    LEFT JOIN visa v ON v.type = i.visatype\n",
    "    WHERE cc.country NOT LIKE 'INVALID:%'\n",
    "    AND cc.country NOT LIKE 'No Country Code%'\n",
    "    AND i.gender IS NOT NULL\n",
    "    LIMIT 10\n",
    "''').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visatype</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WT</td>\n",
       "      <td>1309059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2</td>\n",
       "      <td>1117897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WB</td>\n",
       "      <td>282983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>212410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMT</td>\n",
       "      <td>89133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1</td>\n",
       "      <td>39016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E2</td>\n",
       "      <td>19383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CP</td>\n",
       "      <td>14758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E1</td>\n",
       "      <td>3743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>3176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F2</td>\n",
       "      <td>2984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M1</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I1</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GMB</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M2</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SBP</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CPL</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visatype  count(1)\n",
       "0        WT   1309059\n",
       "1        B2   1117897\n",
       "2        WB    282983\n",
       "3        B1    212410\n",
       "4       GMT     89133\n",
       "5        F1     39016\n",
       "6        E2     19383\n",
       "7        CP     14758\n",
       "8        E1      3743\n",
       "9         I      3176\n",
       "10       F2      2984\n",
       "11       M1      1317\n",
       "12       I1       234\n",
       "13      GMB       150\n",
       "14       M2        49\n",
       "15      SBP        11\n",
       "16      CPL        10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common visa types\n",
    "spark.sql('''\n",
    "    SELECT i.visatype, count(*)\n",
    "    FROM immigration_data i\n",
    "    GROUP BY i.visatype\n",
    "    ORDER BY 2 DESC\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "# counting the number of distinct ports in immigration data\n",
    "spark.sql('''\n",
    "    SELECT DISTINCT i94port\n",
    "    FROM immigration_data\n",
    "''').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many of these ports exist in port of entry codes\n",
    "spark.sql('''\n",
    "    SELECT code\n",
    "    FROM ports\n",
    "    WHERE code IN (SELECT DISTINCT i94port\n",
    "                   FROM immigration_data)\n",
    "''').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>month</th>\n",
       "      <th>avg_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>1</td>\n",
       "      <td>5.310202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>2</td>\n",
       "      <td>7.527418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>3</td>\n",
       "      <td>12.064526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>4</td>\n",
       "      <td>16.999969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>5</td>\n",
       "      <td>21.741758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>Yonkers</td>\n",
       "      <td>8</td>\n",
       "      <td>21.304709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>Yonkers</td>\n",
       "      <td>9</td>\n",
       "      <td>17.081854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>Yonkers</td>\n",
       "      <td>10</td>\n",
       "      <td>10.563062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>Yonkers</td>\n",
       "      <td>11</td>\n",
       "      <td>4.565635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>Yonkers</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.244421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2976 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city  month   avg_temp\n",
       "0     Abilene      1   5.310202\n",
       "1     Abilene      2   7.527418\n",
       "2     Abilene      3  12.064526\n",
       "3     Abilene      4  16.999969\n",
       "4     Abilene      5  21.741758\n",
       "...       ...    ...        ...\n",
       "2971  Yonkers      8  21.304709\n",
       "2972  Yonkers      9  17.081854\n",
       "2973  Yonkers     10  10.563062\n",
       "2974  Yonkers     11   4.565635\n",
       "2975  Yonkers     12  -1.244421\n",
       "\n",
       "[2976 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average temperature of cities in the US\n",
    "temperature_df.createOrReplaceTempView('temperature')\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT city, MONTH(dt) month, AVG(averagetemperature) avg_temp\n",
    "    FROM temperature\n",
    "    WHERE LOWER(country) = 'united states'\n",
    "    AND averagetemperature IS NOT NULL\n",
    "    GROUP BY city, month\n",
    "    ORDER BY city, month\n",
    "''').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------+\n",
      "|code|       city|   state|\n",
      "+----+-----------+--------+\n",
      "| ANC|  ANCHORAGE|      AK|\n",
      "| MOB|     MOBILE|      AL|\n",
      "| LIA|LITTLE ROCK|AR (BPS)|\n",
      "| NOG|    NOGALES|      AZ|\n",
      "| PHO|    PHOENIX|      AZ|\n",
      "+----+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# removing ports where state is null and country is not US\n",
    "clean_ports = spark.sql('''\n",
    "    SELECT code,\n",
    "           SPLIT(location, ',')[0] city,\n",
    "           TRIM(SPLIT(location, ',')[1]) state\n",
    "    FROM ports\n",
    "    WHERE TRIM(SPLIT(location, ',')[1]) IS NOT NULL\n",
    "    AND LOWER( SPLIT(location, ',')[0]) IN (SELECT DISTINCT LOWER(city)\n",
    "                                            FROM temperature\n",
    "                                            WHERE LOWER(country) = 'united states')\n",
    "    ''')\n",
    "\n",
    "clean_ports.createOrReplaceTempView('clean_ports')\n",
    "clean_ports.show(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 3096313\n",
      "0 rows removed\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks\n",
    "# Immigration data\n",
    "immigration_df.createOrReplaceTempView('immigration_data')\n",
    "country_codes_df.createOrReplaceTempView('country_codes')\n",
    "\n",
    "pre_clean_rows = immigration_df.count()\n",
    "print(f'row count: {pre_clean_rows}')\n",
    "\n",
    "immigration_df = spark.sql('''\n",
    "    SELECT i.*, INT(i.i94res) AS country\n",
    "    FROM immigration_data i\n",
    "    JOIN country_codes cc ON INT(i.i94res) = cc.code\n",
    "    WHERE cc.country NOT LIKE 'INVALID:%'\n",
    "    AND cc.country NOT LIKE 'No Country Code%'\n",
    "''')\n",
    "\n",
    "rows_removed = immigration_df.count() - pre_clean_rows\n",
    "\n",
    "print(f'{rows_removed} rows removed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 3096313\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicate records\n",
    "immigration_df.dropDuplicates()\n",
    "\n",
    "# removing unnecessary columns\n",
    "cols = [\n",
    "    'biryear', 'count', 'dtaddto', 'dtadfile',\n",
    "    'entdepa', 'entdepd', 'entdepu',\n",
    "    'i94yr', 'i94mon', 'i94cit', 'i94addr', 'i94res',\n",
    "    'matflag', 'visapost',\n",
    "]\n",
    "immigration_df = immigration_df.drop(*cols)\n",
    "print(f'row count: {immigration_df.count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 03:09:08 WARN SimpleFunctionRegistry: The function isoformat replaced a previously registered function.\n",
      "row count: 3096313\n"
     ]
    }
   ],
   "source": [
    "# renaming columns, casting data types and formating values\n",
    "immigration_df.createOrReplaceTempView('immigration_data')\n",
    "epoch = datetime.datetime(1960, 1, 1).date()\n",
    "\n",
    "spark.udf.register(\n",
    "    'isoformat',\n",
    "    lambda x: (epoch + datetime.timedelta(x)).isoformat() if x else None\n",
    ")\n",
    "\n",
    "immigration_df = spark.sql('''\n",
    "    SELECT INT(i.cicid) AS id,\n",
    "           i.i94port AS port,\n",
    "           i.country AS country,\n",
    "           isoformat(int(i.arrdate)) AS arrival_date,\n",
    "           isoformat(int(i.depdate)) AS departure_date,\n",
    "           i.i94mode AS mode,\n",
    "           i.i94visa AS visa,\n",
    "           i.visatype,\n",
    "           i.airline,\n",
    "           INT(i.i94bir) AS age,\n",
    "           i.gender,\n",
    "           i.fltno AS flight_num,\n",
    "           i.admnum AS admission_num,\n",
    "           i.occup AS occupation\n",
    "      FROM immigration_data i\n",
    "''')\n",
    "\n",
    "print(f'row count: {immigration_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature data\n",
    "\n",
    "# Filtering cities in the US only\n",
    "# aggregating by month\n",
    "temperature_df.createOrReplaceTempView(\"temperature\")\n",
    "\n",
    "temperature_df = spark.sql('''\n",
    "    SELECT city, MONTH(dt) month, AVG(averagetemperature) avg_temp\n",
    "    FROM temperature\n",
    "    WHERE LOWER(country) = 'united states'\n",
    "    AND averagetemperature IS NOT NULL\n",
    "    GROUP BY city, country, month\n",
    "    ORDER BY city, country, month\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 388\n"
     ]
    }
   ],
   "source": [
    "# airlines data\n",
    "\n",
    "# filter out invalid rows\n",
    "# rename columns\n",
    "# select only the ones in i94 immigration data\n",
    "\n",
    "airlines_df.createOrReplaceTempView('airlines')\n",
    "\n",
    "airlines_df = spark.sql('''\n",
    "    SELECT a.name,\n",
    "           a.iata code,\n",
    "           a.country\n",
    "    FROM airlines a\n",
    "    WHERE a.iata IS NOT NULL\n",
    "    AND a.iata IN (SELECT DISTINCT airline\n",
    "                   FROM immigration_data)\n",
    "    AND a.iata <> '-'\n",
    "''')\n",
    "\n",
    "print(f'row count: {airlines_df.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step3'></a>\n",
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The data model was designed using star schema, i.e., it contains a fact table and dimension table. It was chosen as it is more performant for data analysis and reporting tools than the traditional relational model.\n",
    "\n",
    "##### Fact\n",
    "`arrivals` - data from i94 immigration data\n",
    " - id\n",
    " - airport\n",
    " - arrival_date\n",
    " - departure_date\n",
    " - mode\n",
    " - visa\n",
    " - visatype\n",
    " - age\n",
    " - gender\n",
    " - airline\n",
    " - flight_num\n",
    " - occupation\n",
    " - admission_num\n",
    " - country\n",
    "\n",
    "##### Dimensions \n",
    "`visa` - visa types\n",
    "  - type\n",
    "  - purpose\n",
    "  - code\n",
    "  - description\n",
    "\n",
    "\n",
    "`arrival_mode` - arrival mode\n",
    "  - code\n",
    "  - desc\n",
    "\n",
    "\n",
    "`calendar` - date dimension\n",
    "  - date\n",
    "  - day\n",
    "  - week_day\n",
    "  - month\n",
    "  - month_name\n",
    "  - quarter\n",
    "  - year\n",
    "  - season\n",
    "  - season_name\n",
    "\n",
    "\n",
    "`ports` - ports of entry codes\n",
    "  - code\n",
    "  - city\n",
    "  - state\n",
    "\n",
    "\n",
    "`airlines` - airlines information\n",
    "  - name\n",
    "  - code\n",
    "  - country\n",
    "\n",
    "\n",
    "`countries` - countries dimension\n",
    "  - code\n",
    "  - country\n",
    "\n",
    "\n",
    "`temperatures` - average temperatures of cities in the US\n",
    "  - city\n",
    "  - month\n",
    "  - avg_temp \n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "- Extract the data from the various data sources and load them into pySpark dataframes\n",
    "- Transform the data using pySpark.sql\n",
    "- Load the data into the proposed star schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step4'></a>\n",
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "- This step was already done in previous steps:\n",
    "    - Data is already loaded into dataframes\n",
    "    - Data was already transformed using pySpark.sql\n",
    "    - `calendar` and `arrival_mode` still need to be created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating the date dimension (a.k.a. calendar)\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2000-01-01', '2030-12-31')\n",
    "})\n",
    "\n",
    "df['day'] = df.date.dt.day\n",
    "df['week_day'] = df.date.dt.day_name()\n",
    "df['month'] = df.date.dt.month\n",
    "df['month_name'] = df.date.dt.month_name()\n",
    "df['quarter'] = df.date.dt.quarter\n",
    "df['year'] = df.date.dt.year\n",
    "\n",
    "df['season'] = df.date.dt.month % 12 // 3 + 1\n",
    "df['season_name'] = df['season'].map(\n",
    "    {1: 'winter', 2: 'spring', 3: 'summer', 4: 'fall'})\n",
    "\n",
    "calendar_schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('day', IntegerType(), True),\n",
    "    StructField('week_day', StringType(), True),\n",
    "    StructField('month', IntegerType(), True),\n",
    "    StructField('month_name', StringType(), True),\n",
    "    StructField('quarter', IntegerType(), True),\n",
    "    StructField('year', IntegerType(), True),\n",
    "    StructField('season', IntegerType(), True),\n",
    "    StructField('season_name', StringType(), True)\n",
    "])\n",
    "\n",
    "calendar_df = spark.createDataFrame(df, schema=calendar_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# arrival_mode dimension\n",
    "data = [(1, 'Air'), (2, 'Sea'), (3, 'Land'), (9, 'Not Reported')]\n",
    "\n",
    "# create RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "arrival_mode_df = rdd.toDF(['code', 'description'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we generate parquet files based on the transformed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# arrivals\n",
    "immigration_df.repartition(40).write.parquet('./data/output/arrivals/', mode='overwrite')\n",
    "\n",
    "# visa\n",
    "visa_types_df.write.parquet('./data/output/visa/', mode='overwrite')\n",
    "\n",
    "# arrival_mode\n",
    "arrival_mode_df.write.parquet('./data/output/arrival_mode/', mode='overwrite')\n",
    "\n",
    "# calendar\n",
    "calendar_df.write.parquet('./data/output/calendar/', mode='overwrite')\n",
    "\n",
    "# ports\n",
    "ports_of_entry_df.write.parquet('./data/output/ports/', mode='overwrite')\n",
    "\n",
    "# airlines\n",
    "airlines_df.write.parquet('./data/output/airlines/', mode='overwrite')\n",
    "\n",
    "# countries\n",
    "country_codes_df.write.parquet('./data/output/countries/', mode='overwrite')\n",
    "\n",
    "# temperatures\n",
    "temperature_df.write.parquet('./data/output/temperatures/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Verify if dataframes have data after previous steps were executed\n",
    " * Verify if parquet files were created correctly and are in the right directory\n",
    " * Compare row count of parquet files and dataframes\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No empty dataframes!\n",
      "All tables were created successfully\n",
      "All parquet files passed the check!\n",
      "All quality checks passed\n"
     ]
    }
   ],
   "source": [
    "import os.path as path\n",
    "\n",
    "tables = [\n",
    "    ('arrivals', immigration_df),\n",
    "    ('visa', visa_types_df),\n",
    "    ('arrival_mode', arrival_mode_df),\n",
    "    ('ports', ports_of_entry_df),\n",
    "    ('temperatures', temperature_df),\n",
    "    ('airlines', airlines_df),\n",
    "    ('countries', country_codes_df),\n",
    "    ('calendar', calendar_df)\n",
    "]\n",
    "\n",
    "# Verify if dataframes have data\n",
    "# Verify if parquet files were created correctly\n",
    "# Compare row count of parquet files and dataframes\n",
    "dataframes_ok = True\n",
    "tables_ok = True\n",
    "rows_ok = True\n",
    "\n",
    "for table, df in tables:\n",
    "    if df.count() < 1:\n",
    "        dataframes_ok = False\n",
    "        print(f'dataframe {df} is empty')\n",
    "\n",
    "    if not path.isfile(f'./data/output/{table}/_SUCCESS'):\n",
    "        tables_ok = False\n",
    "        print(f'Table {table} was NOT created successfuly')\n",
    "\n",
    "    parquet_rows = spark.read.parquet(\n",
    "        f'./data/output/{table}').count()\n",
    "    if parquet_rows != df.count():\n",
    "        rows_ok = False\n",
    "        print(f'Row count for {table} does not match with the number of rows in the dataframe')\n",
    "\n",
    "if dataframes_ok:\n",
    "    print('No empty dataframes!')\n",
    "\n",
    "if tables_ok:\n",
    "    print('All tables were created successfully!')\n",
    "\n",
    "if rows_ok:\n",
    "    print('All parquet files passed the check!')\n",
    "\n",
    "if dataframes_ok and tables_ok and rows_ok:\n",
    "    print('All quality checks passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary can be found [here](data_dictionary.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='step5'></a>\n",
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "    - Python 3.8 was chosen as it is a stable version of one of the most used programming languages. Python is very popularly used in data-driven projects as it contains a vast amount of libraries and functions designed for this purpose.\n",
    "\n",
    "    - PySpark was chosen because it provides tools to explore the data in a scalable manner, allowing developers to use SQL to process data in combination with Python.\n",
    "    It also performs better than other Python libraries given that it relies on Apache Spark to do the heavy lifting.\n",
    "\n",
    "    - Parquet files were chosen because they are a good way to compress data and can easily be used by other tools like pandas/pyarrow and robust databases.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "    - The data should be updated everytime a new dataset is released by the US National Tourism Trade Office. Ideally, the data would be updated daily, but given that processing this amount of data may not be at the top of the government's priority and compiling this data must be a laborious process, a monthly if not a biweekly update could already give decision makers some insight and allow them to follow trends with minimum delay.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "    - In this case, ETL scripts could be used to run on a Hadoop cluster. Amazon EMR could be leveraged to avoid the need to handle processing on-premises and S3 buckets could be used to store the parquet files and other related resources. \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - For this scenario, Apache Airflow could be used to automate the workflow and orchestrate the runs and the data processing.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "    - Amazon Redshift could be used to handle all simultaneous connections as it is a stable and reliable database management system and it would still be a part of the same suite of products (AWS)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('capstone_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6edbed45810cd0e67f981f8c41bf8a7d667105834d95c0a12c67fb49a50aebae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
